{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5ab7103-a285-4444-ad0b-0befee6f0a2b",
   "metadata": {},
   "source": [
    "# Demonstate an Actor/Critic class of RL agent\n",
    "We must use two classes of actors and use a different strategy for training the agents than the REINFORCE demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ae73ab5-7387-4600-b596-9bebd3f37f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from hydronet.rl.tf.util import DriverLogger\n",
    "from hydronet.rl.tf.env import SimpleEnvironment\n",
    "from hydronet.rl.rewards.mpnn import MPNNReward\n",
    "from hydronet.rl.tf.agents import ConstrainedRandomPolicy\n",
    "from hydronet.rl.tf.networks import GCPNActorNetwork, GCPNCriticNetwork\n",
    "from hydronet.mpnn.layers import custom_objects\n",
    "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
    "from tf_agents.replay_buffers.tf_uniform_replay_buffer import TFUniformReplayBuffer\n",
    "from tf_agents.drivers.dynamic_episode_driver import DynamicEpisodeDriver\n",
    "from tf_agents.policies.policy_saver import PolicySaver\n",
    "from tf_agents.agents import PPOClipAgent\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364e85d1-61bd-4a27-ab1f-b5e79eeb8514",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b453c5d6-a724-409b-9be3-54eed6ee2a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpnn_path = Path() / '..' / '..' / 'challenge-2' / 'best-model' / 'best_model.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2d6374-b65e-4574-a394-80b6bbfcb9c2",
   "metadata": {},
   "source": [
    "## Make the Environment\n",
    "Use the energy of the cluster predicted with our MPNN as a reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e47dec9-9646-4421-bdc5-dd85dbdde3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(mpnn_path, custom_objects=custom_objects)\n",
    "reward = MPNNReward(model, per_water=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c93ba31-5598-4db3-89b7-1684db286572",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SimpleEnvironment(maximum_size=10, reward=reward, only_last=False)\n",
    "tf_env = TFPyEnvironment(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60dee7a7-8f6c-45b7-938a-b250c4e142c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': array(1., dtype=float32),\n",
       " 'observation': {'allowed_actions': array([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32),\n",
       "                 'atom': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "                 'bond': array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0], dtype=int32),\n",
       "                 'connectivity': array([[ 0,  1],\n",
       "       [ 1,  0],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11],\n",
       "       [11, 11]], dtype=int32),\n",
       "                 'is_atom': array([ True,  True, False, False, False, False, False, False, False,\n",
       "       False, False, False]),\n",
       "                 'n_atoms': array(2, dtype=int32),\n",
       "                 'n_bonds': array(2, dtype=int32)},\n",
       " 'reward': array(0., dtype=float32),\n",
       " 'step_type': array(0, dtype=int32)})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step([0, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b258cf80-0bdb-44a5-8f01-7360749a111d",
   "metadata": {},
   "source": [
    "## Build the Policies\n",
    "We are going to compare against a baseline policy that picks random setsp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2afae3f-40fe-42b6-b6db-f07b8330cff5",
   "metadata": {},
   "source": [
    "### Make a Random Policy\n",
    "Using our random policy that constrains guesses to actions that produce valid graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a87db22-df63-4613-9efd-b499661f8aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = ConstrainedRandomPolicy(\n",
    "    time_step_spec=tf_env.time_step_spec(),\n",
    "    action_spec=tf_env.action_spec()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3f1c8a-0279-4343-aa94-dca87c07afd0",
   "metadata": {},
   "source": [
    "### Make the RL Agent\n",
    "Use the `GCPNActorNetwork` to predict actions and the `GCPNCriticNetwork` to score values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81181701-25f1-453a-b267-2a8455fa4961",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_net = GCPNActorNetwork(tf_env.observation_spec(), tf_env.action_spec(), tf_env.reset(), \n",
    "                             num_messages=8, node_features=64, graph_features=False,\n",
    "                            output_layers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a18f5ce-5bdc-4fc8-af1e-85a28290a43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_net = GCPNCriticNetwork(tf_env.observation_spec(), tf_env.reset(), num_messages=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e87ec55-9124-4a9d-bf71-729f01d6ff01",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_agent = PPOClipAgent(\n",
    "    tf_env.time_step_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    actor_net=actor_net,\n",
    "    value_net=critic_net,\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    normalize_observations=False,\n",
    "    entropy_regularization=1e-4,\n",
    "    discount_factor=1.,\n",
    ")\n",
    "tf_agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21dbb87-98fc-4b9f-8ce8-9211a908c810",
   "metadata": {},
   "source": [
    "## Drive it to Make some Data\n",
    "Use the TFUniformReplyBuffer to keep track of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4fbfb5b-7e40-44b9-8073-2e9de89346ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = TFUniformReplayBuffer(\n",
    "    tf_agent.collect_data_spec,\n",
    "    batch_size=1,\n",
    "    max_length=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cd7631c-a0ad-4909-879a-3aaae4a82ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "move_logger = DriverLogger('moves.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eae4ef02-8972-4e58-aca7-1e26703e79e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = DynamicEpisodeDriver(tf_env, tf_agent.collect_policy, [buffer.add_batch, move_logger], num_episodes=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d195034-f612-4120-9ed4-3ea3d138535b",
   "metadata": {},
   "source": [
    "Function to measure the average return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d3d0d03-9bb6-4e3c-b5e2-adfc2cd935df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334875d0-fd2a-40e8-b731-481dc95ecb8b",
   "metadata": {},
   "source": [
    "Get the average return for a random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72317b68-05f9-4bc9-a03e-ad11cfeee095",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lward/miniconda3/envs/hydronet/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py:588: UserWarning: Input dict contained keys ['n_waters', 'n_atoms', 'n_bonds', 'bond_graph_indices'] which did not match any model input. They will be ignored by the model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random return: 86.4\n",
      "CPU times: user 1.52 s, sys: 0 ns, total: 1.52 s\n",
      "Wall time: 1.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "random_return = compute_avg_return(tf_env, random_policy, 64)\n",
    "print(f'Random return: {random_return:.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4868b4de-d276-4a76-b198-456bf217f512",
   "metadata": {},
   "source": [
    "And the agent before we train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07758843-e622-4a18-848d-3a26c633a7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained Agent return with randomized policy: 86.6\n",
      "CPU times: user 3.75 s, sys: 66.2 ms, total: 3.82 s\n",
      "Wall time: 3.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "agent_return = compute_avg_return(tf_env, tf_agent.collect_policy, 32)\n",
    "print(f'Untrained Agent return with randomized policy: {agent_return:.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b8944b-6bdc-46f2-aad4-58fc9b72d1ee",
   "metadata": {},
   "source": [
    "Run until we fill the buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6a3da63-cabd-4f4a-b167-fb7fb4b46e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.04 s, sys: 113 ms, total: 9.15 s\n",
      "Wall time: 8.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "state = tf_env.reset()\n",
    "while buffer.num_frames() < buffer.capacity:\n",
    "    state, _ = driver.run(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3f3de1-dc62-4857-8328-6143478024b3",
   "metadata": {},
   "source": [
    "Train over many iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbd0fe4c-8a2d-45c0-ae6f-c2d2b8bf86c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/32 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-2b8537c03f41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Compute the return of the greedy policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mgreedy_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_avg_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mavg_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_avg_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mstep_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'greedy_return'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgreedy_return\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'avg_return'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mavg_return\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-8ad3979f588a>\u001b[0m in \u001b[0;36mcompute_avg_return\u001b[0;34m(environment, policy, num_episodes)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_last\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0maction_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mtime_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mepisode_return\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/ExaLearn/tf-agents/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36maction\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_automatic_state_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m       \u001b[0mpolicy_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_reset_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclip_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/ExaLearn/tf-agents/tf_agents/utils/common.py\u001b[0m in \u001b[0;36mwith_check_resource_vars\u001b[0;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;31m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;31m# autodep-like behavior is already expected of fn.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresource_variables_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMISSING_RESOURCE_VARIABLES_ERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/ExaLearn/tf-agents/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36m_action\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    558\u001b[0m     \"\"\"\n\u001b[1;32m    559\u001b[0m     \u001b[0mseed_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeedStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msalt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tf_agents_tf_policy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m     \u001b[0mdistribution_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pytype: disable=wrong-arg-types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m     actions = tf.nest.map_structure(\n\u001b[1;32m    562\u001b[0m         \u001b[0;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreparameterized_sampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/ExaLearn/tf-agents/tf_agents/agents/ppo/ppo_policy.py\u001b[0m in \u001b[0;36m_distribution\u001b[0;34m(self, time_step, policy_state, training)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     (distributions, new_policy_state['actor_network_state']) = (\n\u001b[0;32m--> 250\u001b[0;31m         self._apply_actor_network(\n\u001b[0m\u001b[1;32m    251\u001b[0m             time_step, policy_state['actor_network_state'], training=training))\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/ExaLearn/tf-agents/tf_agents/agents/ppo/ppo_policy.py\u001b[0m in \u001b[0;36m_apply_actor_network\u001b[0;34m(self, time_step, policy_state, training)\u001b[0m\n\u001b[1;32m    221\u001b[0m       \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_observation_normalizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     return self._actor_network(\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mtime_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/ExaLearn/tf-agents/tf_agents/networks/network.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m       \u001b[0mnormalized_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"network_state\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mnormalized_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     nest_utils.assert_matching_dtypes_and_inner_shapes(\n",
      "\u001b[0;32m~/miniconda3/envs/hydronet/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1028\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1029\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1030\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/ExaLearn/hydronet/hydronet/rl/tf/networks.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, observations, step_type, network_state)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;31m# Reshape the observations to only have one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouter_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unstack_observations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;31m# Get the allowed actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/ExaLearn/hydronet/hydronet/rl/tf/networks.py\u001b[0m in \u001b[0;36m_unstack_observations\u001b[0;34m(observations)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mobservations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_flatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouter_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hydronet/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 867\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hydronet/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 867\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss = None\n",
    "track = []\n",
    "pbar = tqdm(range(32))\n",
    "for epoch in pbar:\n",
    "    # Compute the return of the greedy policy\n",
    "    greedy_return = compute_avg_return(tf_env, tf_agent.policy, 1)\n",
    "    avg_return = compute_avg_return(tf_env, tf_agent.collect_policy, 32)\n",
    "    step_info = {'greedy_return': greedy_return, 'avg_return': avg_return, 'epoch': epoch}\n",
    "    \n",
    "    # Collect a few episodes using collect_policy and save to the replay buffer.\n",
    "    init_ts = tf_env.reset()\n",
    "    final_ts, _ = driver.run(init_ts)\n",
    "\n",
    "    # Use data from the buffer and update the agent's network.\n",
    "    dataset = buffer.as_dataset(sample_batch_size=64, num_steps=2, num_parallel_calls=4)\n",
    "    for (trajs, _), step in zip(dataset, range(16)):\n",
    "        train_loss = tf_agent.train(trajs)\n",
    "\n",
    "        # Store step information\n",
    "        step_info = {'return': avg_return, 'epoch': epoch}\n",
    "        step_info.update(dict(zip(train_loss.extra._fields, map(float, tuple(train_loss.extra)))))\n",
    "        step_info['loss'] = train_loss.loss.numpy()\n",
    "        step_info['step'] = step\n",
    "\n",
    "        track.append(step_info)\n",
    "        \n",
    "        # Update the progress bar\n",
    "        pbar.set_description(f'loss: {train_loss.loss:.2e} - return: {avg_return:.1f} - step: {step}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136acf12-82fe-4127-ae30-4c4e7aaf6ea1",
   "metadata": {},
   "source": [
    "Measure the performance after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0174632a-63f0-421a-b066-872bf7ec772a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_return = compute_avg_return(tf_env, tf_agent.collect_policy, 32)\n",
    "print(f'Agent return with randomized policy: {agent_return:.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad23c2ca-2b57-435a-b0b4-070493df4449",
   "metadata": {},
   "source": [
    "## Save the Model and Training Performance\n",
    "So we can analyze them later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434f7cfa-1b08-4b61-a591-1bc18a4a24de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_info = pd.DataFrame(track)\n",
    "train_info.to_csv('train_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffb08b4-4438-4ed9-863d-ef8a35355f1f",
   "metadata": {},
   "source": [
    "## Make a Simple Plot\n",
    "See if the return is increasing over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f92874f-d11d-44a7-9c95-517cac1b0ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(train_info['return'])\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Return')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1a698a-e4d1-4662-887a-4020492c9a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(train_info['loss'])\n",
    "\n",
    "ax.set_yscale('symlog')\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1edf24-a6e4-494d-8f26-edd80b4c6a1b",
   "metadata": {},
   "source": [
    "Plot some trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a68ce30-998d-49f6-8d45-76c9b88140a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectories(environment, policy, num_episodes=10) -> pd.DataFrame:\n",
    "    \"\"\"Get trajectory of energy wrt step\n",
    "    \n",
    "    Args:\n",
    "        environment: Water cluster environment\n",
    "        policy: Policy to execute\n",
    "    Returns:\n",
    "        List of trajectories\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    for e in range(num_episodes):\n",
    "        time_step = environment.reset()\n",
    "        step = 0\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            graph = tf_env.pyenv.envs[0].get_state()\n",
    "            output.append({\n",
    "                'episode': e,\n",
    "                'step': step,\n",
    "                'graph': graph,\n",
    "                'energy': -reward(graph),\n",
    "                'size': len(graph),\n",
    "                'reward': float(-1 * time_step.reward.numpy()),\n",
    "            })\n",
    "            step += 1\n",
    "\n",
    "    return pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51c50e2-826d-4573-af30-7027797d540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rl_traj = get_trajectories(tf_env, tf_agent.collect_policy, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9cd58c-a271-4423-9f25-2c3d610ba1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "greedy_traj = get_trajectories(tf_env, tf_agent.policy, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c34971-016c-469a-be08-5173a9f7a444",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "random_traj = get_trajectories(tf_env, random_policy, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5526e7bb-9e09-4a08-b2b2-46dbf677c45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3.5, 2.2))\n",
    "\n",
    "best_energy_rl = rl_traj.query('size<=10').groupby('episode').min()['energy']\n",
    "best_energy_rand = random_traj.query('size<=10').groupby('episode')['energy'].min()\n",
    "print(f'Lowest energy from RL: {min(best_energy_rl):.2f}')\n",
    "print(f'Lowest energy from Random: {min(best_energy_rand):.2f}')\n",
    "bins = np.linspace(\n",
    "    min(best_energy_rand.min(), best_energy_rl.min()) * 1.1,\n",
    "    max(best_energy_rand.max(), best_energy_rl.max()) * 0.9,\n",
    "    32\n",
    ")\n",
    "\n",
    "ax.hist(best_energy_rl.values, bins=bins, color='r', label='RL')\n",
    "ax.hist(best_energy_rand, bins=bins, alpha=0.5, color='b', label='Random')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlabel('Energy (Ha)')\n",
    "ax.set_ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4dabba-9544-46ad-8208-70c9753efcbd",
   "metadata": {},
   "source": [
    "Save solutions to disk for later analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2945b4fa-f5ce-4e04-8c14-52008a023639",
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_traj.to_pickle('rl_outputs.pkl')\n",
    "random_traj.to_pickle('random_outputs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d54a5b0-0942-40ce-85c9-da8a3e231144",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
