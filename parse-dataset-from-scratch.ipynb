{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the Data, Make Pandas-able\n",
    "The raw data for this project is an ASE Database that holds the results of the water cluster calculations taken from a recent paper by [Rakshit et al.](https://doi.org/10.1063/1.5128378).\n",
    "We need to convert these ASE `Atoms` objects, which list the atomic coordinates and energy, into a form with the graph structure.\n",
    "This notebook contains the code to compute the graph structure for each entry in the database and save it all in an easily-accessible Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphsage.importing import create_graph, make_entry, make_tfrecord, make_nfp_network\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "from ase.io.xyz import write_xyz\n",
    "from ase.db import connect\n",
    "from random import Random\n",
    "from tqdm import tqdm\n",
    "from io import StringIO\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import json\n",
    "import gzip\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_graphs = 10 ** 3  # Number of graphs to read out from database for DF sample\n",
    "val_fraction = 0.05  # Fraction of entries set aside for early stopping\n",
    "test_fraction = 0.05  # Fraction of entries set aside for model testing\n",
    "n_jobs = min(8, os.cpu_count())  # Number of processes to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the ASE database from the ZIP file\n",
    "We are going to uncompress it temporarily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_zip = zipfile.ZipFile(os.path.join('data', 'input', 'ALL_geoms.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46 µs, sys: 25 µs, total: 71 µs\n",
      "Wall time: 44.8 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_path = os.path.join('temp.db', 'water_db', 'ALL_geoms_all_sorted.db')\n",
    "if not os.path.isfile(data_path):\n",
    "    path_check = data_zip.extract('water_db/ALL_geoms_all_sorted.db', 'temp.db')\n",
    "    assert path_check == data_path\n",
    "    print(f'Extracted data to {data_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to database with 4464740 records\n",
      "CPU times: user 23.2 ms, sys: 34.9 ms, total: 58.1 ms\n",
      "Wall time: 58.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ase_db = connect(data_path)\n",
    "print(f'Connected to database with {len(ase_db)} records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert ASE Objects to Networkx Graphs\n",
    "The code here is adapated from the [Exalearn:Design Github page](https://github.com/exalearn/design/blob/16cfe21d85528c6004514d2985428566453b24a1/graph_descriptors/graph_builder.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over the whole database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_from_database(ase_db, rng, chunk_size=128, total=None):\n",
    "    \"\"\"Iterate over a large database\n",
    "    \n",
    "    Queries only a small chunk size at a time to prevent loading the \n",
    "    whole database into memory. \n",
    "    \n",
    "    Args:\n",
    "        ase_db (Connection): Connection to an ASE database\n",
    "        rng (np.Random): Random number generator used to shuffle data\n",
    "        chunk_size (int): Number of entries to retrieve per query\n",
    "        total (int): Total number of entries to retrieve\n",
    "    \"\"\"\n",
    "    # Figure out how many iterations we need to make\n",
    "    if total is None:\n",
    "        total = ase_db.count()\n",
    "    \n",
    "    # Generate the dataset\n",
    "    starts = np.arange(0, total, chunk_size, dtype=np.int32)\n",
    "    \n",
    "    # Randomize the starts to help the diversity\n",
    "    if rng is not None:\n",
    "        rng.shuffle(starts)\n",
    "    \n",
    "    # Iterate through the whole database\n",
    "    for start in starts:\n",
    "        for a in ase_db.select(\n",
    "            selection=[('id','>=', str(start)), ('id', '<', str(start+chunk_size))]):\n",
    "            yield a.toatoms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1023it [00:01, 642.13it/s]                                                                                                                                                                               \n"
     ]
    }
   ],
   "source": [
    "with Pool(n_jobs - 1) as p:  # Keep one thread reading the database\n",
    "    graphs = list(tqdm(\n",
    "        p.imap(make_entry, pull_from_database(ase_db, rng, total=total_graphs), chunksize=64),\n",
    "        total=total_graphs\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = pd.DataFrame(graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>graph</th>\n",
       "      <th>energy</th>\n",
       "      <th>n_waters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>-40.522038</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>-40.499618</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               graph     energy  n_waters\n",
       "0  (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,... -40.522038         6\n",
       "1  (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,... -40.499618         6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphs.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the DataFrame\n",
    "Save the dataframe to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs.to_pickle(os.path.join('data', 'output', 'water_clusters.pkl.gz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Data as NFP-ready TensorFlow record objects\n",
    "Save the whole file as a JSON-LD where each entry has the form:\n",
    "\n",
    "```json\n",
    "{\n",
    "  'entry': 'entry number as an integer',\n",
    "  'energy': 'energy as a float',\n",
    "  'n_waters': 'number of water molecules as an integer', \n",
    "  'n_atom': 'number of atoms as an integer', \n",
    "  'n_bonds': 'number of bonds as an integer',\n",
    "  'atom': 'List of atom types (0 -> Oxygen, 1 -> Hydrogen)',\n",
    "  'bond': 'List of bond types (0 -> covalent, 1 -> Hydrogen)',\n",
    "  'connectivity': 'List of connections between atoms, as a list of pairs of ints. Sorted ascending by column 0, then 1'\n",
    "  'xyz': 'XYZ format version of the whole file.'\n",
    "}\n",
    "```\n",
    "\n",
    "Also save a \"coarsened\" version of the network with a single node per water. \n",
    "Bonds are directional in the coarsened version, with a \"donor\" and \"acceptor\" bond type.\n",
    "The format is otherwise identical but lacks the 'xyz' field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull a single cluster and make its network graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "atoms = next(pull_from_database(ase_db, None, total=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_water': 3,\n",
       " 'n_atom': 9,\n",
       " 'n_bond': 18,\n",
       " 'atom': [0, 1, 1, 0, 1, 1, 0, 1, 1],\n",
       " 'bond': [0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1],\n",
       " 'connectivity': [[0, 1],\n",
       "  [0, 2],\n",
       "  [0, 8],\n",
       "  [1, 0],\n",
       "  [1, 3],\n",
       "  [2, 0],\n",
       "  [3, 1],\n",
       "  [3, 4],\n",
       "  [3, 5],\n",
       "  [4, 3],\n",
       "  [4, 6],\n",
       "  [5, 3],\n",
       "  [6, 4],\n",
       "  [6, 7],\n",
       "  [6, 8],\n",
       "  [7, 6],\n",
       "  [8, 0],\n",
       "  [8, 6]],\n",
       " 'energy': -15.9416428}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_nfp_network(atoms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can optionally coarsen that graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_water': 3,\n",
       " 'n_atom': 3,\n",
       " 'n_bond': 6,\n",
       " 'atom': [0, 0, 0],\n",
       " 'bond': [0, 1, 1, 0, 1, 0],\n",
       " 'connectivity': [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]],\n",
       " 'energy': -15.9416428}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_nfp_network(atoms, coarsen=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or just express the geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'z': [8, 1, 1, 8, 1, 1, 8, 1, 1],\n",
       " 'n_water': 3,\n",
       " 'n_atom': 9,\n",
       " 'atom': [0, 1, 1, 0, 1, 1, 0, 1, 1],\n",
       " 'coords': [[25.3875809, 2.28446364, 8.01933861],\n",
       "  [24.686451, 2.11461496, 7.36908007],\n",
       "  [26.1070786, 1.70453322, 7.77935553],\n",
       "  [22.9643402, 1.68695939, 6.75715494],\n",
       "  [22.7494984, 1.67431045, 7.70416498],\n",
       "  [22.2382431, 2.13693213, 6.33168697],\n",
       "  [23.0780773, 1.86950338, 9.5477314],\n",
       "  [22.9238548, 2.4637537, 10.2781725],\n",
       "  [23.9850082, 2.04813766, 9.2500248]]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def geometry_record(atoms):\n",
    "    \"\"\"Create a JSON-ready record with the geometry and atomic types\n",
    "    \n",
    "    Args:\n",
    "        atoms: ASE atoms object\n",
    "    Returns:\n",
    "        dictionary that can be easily serialized to JSON\n",
    "    \"\"\"\n",
    "    z = atoms.get_atomic_numbers()\n",
    "    return {\n",
    "        'z': z.tolist(),\n",
    "        'n_water': len(z) // 3,\n",
    "        'n_atom': len(z),\n",
    "        'atom': list(map([8, 1].index, atoms.get_atomic_numbers())),\n",
    "        'coords': atoms.get_positions().tolist()\n",
    "    }\n",
    "geometry_record(atoms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate many records in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_split = np.random.RandomState(4)\n",
    "rng_pull = np.random.RandomState(129)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4464740/4464740 [8:49:53<00:00, 77.59it/s]"
     ]
    }
   ],
   "source": [
    "# Create the output files\n",
    "filenames = [\n",
    "    'geom_train', 'geom_test', 'geom_valid',\n",
    "    'atomic_train', 'atomic_test', 'atomic_valid',\n",
    "    'coarse_train', 'coarse_test', 'coarse_valid'\n",
    "]\n",
    "make_output = lambda x: tf.io.TFRecordWriter(os.path.join('data', 'output', f'{x}.proto'))\n",
    "output_files = dict((x, make_output(x)) for x in filenames)\n",
    "make_output = lambda x: gzip.open(os.path.join('data', 'output', f'{x}.json.gz'), 'wt')\n",
    "json_outputs = dict((x, make_output(x)) for x in filenames)\n",
    "\n",
    "# Control functions\n",
    "batch_size = 4096\n",
    "entry_gen = pull_from_database(ase_db, rng_pull)\n",
    "counter = tqdm(total=len(ase_db))\n",
    "coarse_fun = partial(make_nfp_network, coarsen=True)\n",
    "\n",
    "try:\n",
    "    done = False\n",
    "    with Pool(n_jobs - 1) as p:  # One CPU open for serialization\n",
    "        while not done:\n",
    "            # Get the next batch of entries \n",
    "            batch = []\n",
    "            for _ in range(batch_size):\n",
    "                try:\n",
    "                    batch.append(next(entry_gen))\n",
    "                except StopIteration:\n",
    "                    done = True\n",
    "                    break\n",
    "            \n",
    "            # Make the random choices\n",
    "            split_rnd = rng_split.random(len(batch))\n",
    "            \n",
    "            # Save the geometries\n",
    "            name = 'geom'\n",
    "            for atoms, r in zip(batch, split_rnd):\n",
    "                # Make the record\n",
    "                entry = geometry_record(atoms)\n",
    "                serial_entry = make_tfrecord(entry)\n",
    "                \n",
    "                # Store in a specific dataset\n",
    "                if r < val_fraction:\n",
    "                    out_name = f'{name}_valid'\n",
    "                elif r < val_fraction + test_fraction:\n",
    "                    out_name = f'{name}_test'\n",
    "                else:\n",
    "                    out_name = f'{name}_train'\n",
    "                    \n",
    "                # Save to file\n",
    "                output_files[out_name].write(serial_entry)\n",
    "                print(json.dumps(entry), file=json_outputs[out_name])    \n",
    "                \n",
    "            \n",
    "            # Process for both atomic and coarse_network\n",
    "            for name, func in zip(['atomic', 'coarse'], [make_nfp_network, coarse_fun]):\n",
    "                for atoms, entry, r in zip(batch, p.imap(func, batch, chunksize=64), split_rnd):\n",
    "                    # Serialize the entry\n",
    "                    serial_entry = make_tfrecord(entry)\n",
    "\n",
    "                    # Store in a specific dataset\n",
    "                    if r < val_fraction:\n",
    "                        out_name = f'{name}_valid'\n",
    "                    elif r < val_fraction + test_fraction:\n",
    "                        out_name = f'{name}_test'\n",
    "                    else:\n",
    "                        out_name = f'{name}_train'\n",
    "                    \n",
    "                    # Save to file\n",
    "                    output_files[out_name].write(serial_entry)\n",
    "                    print(json.dumps(entry), file=json_outputs[out_name])\n",
    "                        \n",
    "            # Update TQDM\n",
    "            counter.update(len(batch))\n",
    "finally:\n",
    "    for out in output_files.values():\n",
    "        out.close()\n",
    "    for out in json_outputs.values():\n",
    "        out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
